= Design and Implementation of a Fast Logging System
:toc:
:toc-placement!:

toc::[]

# Introduction

This document records the lessons I have learnt in designing, implementing, and benchmarking a fast logging system. The motivation of a fast logging system is based on a simple observation: data race detection using a linear program trace is an embarassingly parallel problem. One can simply divide the program trace into many chunks and run data race analysis on each of the chunk independently. Thus, I believe that, by decoupling logging from the more expensive race detection algorithm, we can engineer RV-Predict so that its overall overhead is essentially the overhead of logging (when given enough compute resources), regardless of the race detection algorithm used in the analysis backend.

# Design Goals

The two most important design goals for the logging system are *high throughput*, measured in the number of events logged per second per core, and *linear scalability* (i.e., per-core throughput unaffected as you increase the number of cores logging). It would also be nice to have a *predictable per-event logging overhead* (i.e., the reciprocal throughput measured in cycles/event) so that 1) the overall slowdown can be estimated easily, and 2) there will be no pathological case that renders the system unusable (e.g., the slowdown of TSan can be >1000x for some programs).

In addition, the logging system must be designed for *simplicity*. Complexity should only be justified by significant performance gains (e.g., >10%) backed by concrete numbers. However, it's OK to sacrifice "absolute correctness" in rare cases as long as the behavior is well-defined and it doesn't cripple the race detector.

# Design Overview

This section describes the overall design of the logging system at a high level. The actual implementation might differ in details for performance reasons.

## Concepts

Some concepts used in the rest of this document include:

* `*Application threads*` - Threads created by the application under test.

* `*Worker threads*` - Threads created by RV-Predict runtime to run data race detection algorithm.

* `*Epoch*` - A period of time in which events logged are put together to check data race by a worker thread.

* `*Event buffer*` - Buffers used to log events; each application thread is assigned an empty event buffer at the beginning of an epoch.

* `*Coordinator*` - At the end of each epoch, an application thread is selected to become the coordinator that supervises the transition into next epoch.

* `*Buffer manager*` - Singleton object that maintains a pool of free event buffers, the ownership of each allocated event buffer, and the current epoch number.

* `*Thread-local trace*` - A stream of events happened in a particular thread.

* `*Global trace*` - A stream of events happened in all threads of a program (obtained by merging multiple thread-local traces)


## Logging Overview

The key to achieve high-throughput and scalable logging is to avoid coordination between threads at all cost in the common case. Specifically, we shall not use techniques like global atomic counter to order events from different threads. Ideally, we would like the per-core throughput of our logging system to be close to a simplistic logging system designed only for single-threaded programs.

The easiest way to understand the logging algorithm is to walk through the things that happened in an epoch.

At the beginning of each epoch, each thread is assigned an empty event buffer. Each application thread can access its own event buffer via a thread-local pointer:
[source, c++]
----
/// Per-thread event buffer pointer. NULL if no event
/// buffer has been assigned to us.
thread_local EventBuffer* __log_buffer;
----
An event buffer is essentially a fixed-size array where new events are appended to the end:
[source, c++]
----
struct EventBuffer {
    /// Maximum number of events a buffer can hold.
    static const int MAX_EVENTS = 10000000;

    /// # events stored in the buffer.
    int events;

    /// Buffer storage used to hold events.
    Event buf[MAX_EVENTS];

    /// More member variables omitted...
};
----

We chose fixed-size array over, say, ring buffer because a fixed-size array is simpler and likely faster (e.g. can check if buffer is full by comparing against a constant, no need to worry about wrapping around, etc.).

When an event buffer becomes full, the application thread will propose to the buffer manager to start a new epoch (thus obtaining a new event buffer). There can be multiple threads trying to increment the epoch number concurrently but only one can succeed. The application thread that successfully increments the epoch number is selected to become the so-called coordinator, who is responsible for ensuring a smooth transition to the next epoch.

There are two major tasks a coordinator needs to perform during the transition. The first task is to notify other threads about the epoch change. This can be done by writing `NULL` to the event buffer pointers (i.e., `__log_buffer`) of the threads that are participating in the current epoch. And the buffer manager knows exact what these threads are because each application thread has to contact the buffer manager for a new event buffer at the beginning of an epoch.

[NOTE]
==========================
Keyword `thread_local` in C++ doesn't restrict a variable to be only accessed from one thread; the variable can be accessed normally in other threads as long as we have its address.
==========================

When a thread is about to log an event, it has to first check if its current event buffer has been reclaimed (i.e., load the event buffer pointer and see if it's `NULL`) and contact the buffer manager for a new one if necessary:
[source, c++]
----
/// Load the event buffer pointer using atomic operation
/// (so the compiler knows this variable can be accessed
/// from other threads).
EventBuffer* getLogBuffer()
{
    return __atomic_load_n(&__log_buffer, __ATOMIC_RELAXED);
}

// Simplified pseudo-code for logging a new event.
void logEvent(Event event)
{
    EventBuffer* curBuf = getLogBuffer();
    if (curBuf == NULL) {
        curBuf = __buffer_manager.allocBuffer();
    }
    curBuf->buf[curBuf->events++] = event;
}
----

The second task of the coordinator is to reclaim the event buffers used in the old epoch and pass them to a worker thread for processing. However, the coordinator cannot proceed without first synchronizing with other application threads. Otherwise, we cannot prevent the application threads from writing more events into the buffers even though the worker thread has started reading. A simple solution is to introduce a barrier and turn `BufferManager::allocBuffer()` into a blocking function call.

[source, c++]
----
/// TODO: add doc
EventBuffer*
BufferManager::allocBuffer()
{
    // Obtain an empty event buffer (possibly by recycling
    // one from past epochs).
    EventBuffer* newBuf = getFreshBuf();

    // Wait until all application threads of the old epoch
    // have reached here.
    barrier_wait(&epochBarrier);

    // getLogBuf() will return the new buffer from now.
    __log_buffer = newBuf;
    return newBuf;
}
----

However, this simple solution runs the risk of deadlock. For instance, imagine a thread that has no event to log and, thus, won't detect the epoch change. At the very least, it's quite inefficient having to block all application threads participating in the old epoch (e.g., what if some thread is sleeping, what if the number of participating threads is much larger than the number of cores, etc.).

One possible fix would be to block the worker thread instead of application threads. That is, inside `BufferManager::allocBuffer()`, instead of entering a barrier, simply mark the old event buffer as "closed", meaning that the worker thread can safely read from it. The worker thread will then wait until all event buffers are closed before processing them.

All's good except that there is actually another important use of barrier: to enforce cut consistency. Consistent cut is a classic problem in distributed system. In our context, it basically states that if a read `R` from thread `T1` observes the value of a write `W` from another thread `T2` then we cannot log `W` in a later epoch than `R`. To understand why a barrier helps enforce cut consistency, consider the following example (without barrier):

==========================
1. `T1` detects an epoch change when it's about to log `W`
2. `T1` is assigned a new buffer and logs `W` in the new epoch
3. `T1` executes `W`
4. `T2` executes `R` and observes the value written by `W`
5. `T2` logs `R` in the old epoch
6. `T2` does some more work and logs more events
7. `T2` detects an epoch change when it's about to log some new event
==========================

With a barrier, `T1` will not be able to obtain a new event buffer until `T2` has also detected the epoch change. Therefore, even though `W` is logged to a later epoch, its value is not observed by `R` and no consistency is violated. 

So we are in a dilemma here. On one hand, we need the barrier to enforce cut consistency. On the other hand, we really don't want applications threads to block forever (or for a long time). I don't have a perfect solution yet, unfortunately, but I think using a timeout with the barrier could work fine in practice. To be more precise, the first thread arriving at the barrier will set a cancellation deadline (i.e., `currentTime + timeout`), after which all blocking threads will be released. The timeout value we chose should be small enough so that we don't waste too many CPU cycles. For example, if it takes at least 10 ms to fill up an event buffer, setting the timeout value to 100 us means we will waste at most 1% CPU time.

The use of timeout in the barrier greatly complicates the analysis of cut consistency. In the rest of this section, I will try to reason if cut consistency can be violated due to the broken barrier. Following the notation used earlier, let `W` and `R` be the write-read pair that violates the cut consistency. More precisely, `W` is a write operation in thread `T1` whose logging results in the detection of a new epoch and `R` is a read operation in thread `T2` that observes the value of `W` and logged in the old epoch. Based on whether `T1`/`T2` enters the barrier, we can divide the problem into four possible cases. Thus, our goal is to prove that none of the four cases is actually possible (otherwise, cut consistency can be violated).

*Case 1: both `T1` and `T2` enters the barrier.*

This is impossible. `R` cannot observe the value of `W` because of the barrier (as we have shown earlier).

*Case 2: `T1` enters the barrier but not `T2`.*

The fact that `T2` reaches barrier after the timeout suggests that the logging of `R` happens much later (e.g., in the order of 100 us) than the epoch change notification sent out by the coordinator. It's almost certainly `T2` will observe the epoch change during logging `R` and, thus, log `R` into the new epoch, which is conflict with our definition of `R`. However, there is a flaw in the above reasoning: logging of `R` doesn't happen atomicly. There is a slight chance that `T2` could load the old event buffer, get descheduled, and finally resume after the barrier timeout, in which case `R` will be logged to the old epoch. Fortunately, in this case, `R` will be executed much earlier than the barrier timeout and won't be able to observe the value of `W`, which is again conflict with our definition of `R`. The following is an illustration of this corner case. Note that "load buffer pointer" could happen after the barrier starts, but not too much later if it wants to observe the old buffer pointer because our barrier timeout is much larger (e.g., 1000x) than the time for cross-core information propogation.

          T2 observes (old) buffer pointer and gets descheduled       T2 resumes
          |                                                           |
-------+--+------------+--------------------- ... ----------------+---+----
       |               |                                          |
       T2 executse R   barrier starts                             barrier timeout

*Case 3: `T2` enters the barrier but not `T1`.*

This is impossible. `R` cannot observe the value of `W` because it is executed much earlier than `W` (which is only executed after the barrier timeout).

*Case 4: none of them enters the barrier.*

For the same reason as *Case 2*, there is no such `R` that satisfies our definition.

To sum up, by choosing the barrier timeout carefully (i.e., `cross-core communication delay << barrier timeout << min. epoch time`), we can effectively guarantee cut consistency in practice while sacrificing only a tiny fraction of CPU time.

## Merge Thread-Local Traces

*TODO:* _describe how to merge traces using timestamps and R/W values of atomic operations_


# Implementation

## Instrumentation

## Event Layout

## Synchronization

TODO: Between application threads, between work threads and application threads
Use monitor style locking for buffer manager

## Timestamp

TODO: how to take timestamp properly? `lfence; rdtsc; lfence`? `rdtscp; lfence`? cost?

# Benchmark

So far, we haven't really touched on the topic of performance engineering. One approach to develop a fast logging system would be to come up with a simple prototype first and then try to optimize it. However, for this project, I decided to approach it differently in a performance-oriented fashion: I started with a unrealistically simple logging system for single-threaded programs and tried to extend it for multi-threaded programs. The purpose of this decision is actually three-fold:

* Establish a lower bound on the logging overhead (or, a upper bound on throughput)
* Understand the different sources of logging overhead
* Use the stringent performance requirements to narrow the design space from the early stage

To this end, I designed a number of micro-benchmarks which I will walk through in the rest of this section.

## Experiment Setup

All experiments in this section are performed on the CloudLab http://docs.cloudlab.us/hardware.html#%28part._cloudlab-clemson%29[c6420] nodes:
----
CPU         Two Sixteen-core Intel Xeon Gold 6142 CPUs at 2.6 GHz
RAM         384GB ECC DDR4-2666 Memory
OS          Ubuntu 18.04
Compiler    GCC 7.3.0
----

CPU features like TurboBoost, hyperthreading, frequency scaling, and deep C-states are all disabled to ensure a stable environment:

[source, bash]
----
#!/bin/bash

# Set up a stable environment to run performance benchmarks on cloudlab machines.

# Install useful tools
sudo apt update
sudo apt install -y linux-tools-common linux-tools-$(uname -r) cpuset cpufrequtils i7z tuned

# Disable address space randomization.
# Suggested by https://llvm.org/docs/Benchmarking.html
echo 0 | sudo tee -a /proc/sys/kernel/randomize_va_space

# Disable dynamic frequency scaling and deep C-states.
sudo systemctl start tuned
sudo tuned-adm profile latency-performance

# Disable hyper-threading.
maxCPU=$(($(lscpu | grep "^CPU(s)" | awk '{print $2}') - 1))
for n in `seq 0 $maxCPU`
do
    cpu=/sys/devices/system/cpu/cpu$n
    hyperthread=$(cut -d, -f2 $cpu/topology/thread_siblings_list)
    if [[ "$n" = "$hyperthread" ]]
    then
        echo 0 | sudo tee -a $cpu/online
    fi
done

# Disable turbo-boost.
cores=$(cat /proc/cpuinfo | grep processor | awk '{print $3}')
for core in $cores; do
    sudo wrmsr -p${core} 0x1a0 0x4000850089
    state=$(sudo rdmsr -p${core} 0x1a0 -f 38:38)
    if [[ $state -eq 1 ]]; then
        echo "core ${core}: disabled"
    else
        echo "core ${core}: enabled"
    fi
done

# Allow us to collect performance metrics via `perf` without sudo.
echo -1 | sudo tee -a /proc/sys/kernel/perf_event_paranoid
----

In addition, a number of cores on the same NUMA node are isolated using `cpuset` and dedicated to running benchmark programs.

 $ sudo cset shield -c 2,4,6,8,10, -k on
 $ sudo cset shield --exec -- ./startBenchmark [ARGS...]


## Micro-benchmarks

To study the overhead of logging, I take the https://github.com/llvm-mirror/compiler-rt/blob/master/lib/tsan/benchmarks/mini_bench_local.cc[mini_bench_local.cc] example in TSan and manually instrument it with calls to the logging library. Then I implement many variants of the logging function whose bodies range from an empty stub to doing slightly more interesting things like logging the address to a full-fledged implementation. The following table summarizes the result.

.Micro-benchmark Summary
[options="header"]
|=======================
|Variant        |Description                |Cycles^1^|Instructions^2^
|`NO_OP`        |Baseline|2.13|n/a
|`NO_SSE`       |Disable the use of SSE instructions|2.28|4
|`FUNC_CALL`    |Call an empty function (no inlining)|5.56|8
|`LOG_ADDR`     |Log only memory address|4.03|11
|`PREFETCH_LOG_ENTRY`|`LOG_ADDR` + increase event buffer size and prefetch log entries|4.78|11
|`VOLATILE_BUF_PTR` |`LOG_ADDR` + use atomic load to read event buffer pointer|8.27|13
|`CACHED_BUF_PTR`   |`VOLATILE_BUF_PTR` + cache buffer pointer in register|4.13|14
|`LOG_HEADER`   |`LOG_ADDR` + log event header|4.09|13
|`LOG_VALUE`    |`LOG_HEADER` + log R/W values|4.99|17
|`LOG_SRC_LOC`  |`LOG_VALUE` + log source code location|4.41|15
|`LOG_FULL`     |`LOG_SRC_LOC` + `PREFETCH_LOG_ENTRY` + `CACHED_BUF_PTR`|5.87|20
|`LOG_FULL_NAIVE`   |`LOG_FULL` w/o optimizations|9.47|25
|`BUFFER_MANAGER`   |`LOG_FULL` + integration with buffer manager|TODO|TODO
|`TSan`   |Instrumented with ThreadSanitizer|20.07|74
|=======================
*^1^ Average CPU cycles spent per iteration.*

*^2^ Number of instructions of each iteration in the common case.*

ThreadSanitizer is also included for comparison. Its overhead (i.e., extra cycles compared to `NO_OP`) is about *17.94* cycles/iteration, which is almost *5X* of the overhead of `LOG_FULL`, which is only *3.74* cycles/iteration.

*TODO:* _say more about each variant and include source code + assembly code_


## Sources of Overhead

The overhead of logging can arise in unexpected places. It is more than just executing the extra code instrumented. Furthermore, not all instructions instrumented are equal: some are better pipelined than others. In this section, I will walk through the interesting lessons I have learned from analyzing the benchmark results.

*First, instrumentation can render certain compiler optimizations unapplicable.* Consider the following code snippet:
----
int64_t array[100000];
for (int i = 0; i < 100000; i++) {
    array[i] = i;
}
----

Such code can benefit from using SSE instructions. However, this is no longer possible when the code is interleaved with calls to the logging library. Here is another example: suppose we change our definition of `array` to `int[100000]`, now the compiler can merge two `int` writes into one `int64_t` write, reducing the number of store instructions by half. This is also not possible if our instrumentation runs before this optimization pass. In general, we should probably try to run our instrumentation pass after compilation passes that can reduce the number of loads/stores.

*Second, the cost of function call is not negligible.* Experiment `FUNC_CALL` demonstrates that just calling an empty non-inlined function adds *3.28* cycles overhead per iteration! Note that this is actually a real source of overhead for ThreadSanitizer because its runtime function is too heavyweight to be inlined. This should not be a problem for our logging library because our functions are much lighter. We just need to properly implement our functions in header files (with `extern` declarations in source files) and compile our runtime as a static library.


*Third, reading the event buffer pointer is expensive!* In `VOLATILE_BUF_PTR`, we use the atomic load to read the event buffer pointer so that compiler cannot hoist it out of the loop into a local variable (register). I was originally surprised to see that reading the event buffer pointer adds *4.24* cycles overhead to `LOG_ADDR` but then realized that it makes a lot of sense.
[source, c++]
----
// Code snippet of VOLATILE_BUF_PTR
EventBuffer* logBuf = __atomic_load_n(&__log_buffer, __ATOMIC_RELAXED);
curBuf->buf[curBuf->events++] = newEvent;
----
Consider the code example above, the processor has to fetch `curBuf` from L1 cache first before it can fetch `curBuf->{buf, events}` again from L1. There is still some pipelining between the data fetch and other instructions but the latency of L1 cache plus the data dependency probably cause significant stalls in the instruction pipeline. (_TODO: How can we verify this? Maybe Andi Kleen's topLev tool?_)

_TODO: describe the optimization used in CACHED_BUF_PTR, when it's applicable, and its additional requirment on the instrumentation_

*Fourth, recording full information is expensive (both in time and space)!* In the current event layout, we are squeezing everything into a 64-bit integer by recording only the last byte of the value and the lower 32 bits of the memory address. If we were to record the full 64-bit value and all 48 bits of virtual address, we would have to use 128-bit integers, effectively doubling the usage of memory bandwidth. On the other hand, if the backend algorithm permits, it would be more efficient to log only the values of atomic operations.

_TODO: understand why LOG_SRC_LOC is faster than LOG_VALUE_

*Fifth, logging could introduce a lot of cache misses!* In `PREFETCH_LOG_ENTRY`, I increased the size of the event buffer to be much larger than the L3 cache to study the influence of logging on cache. Since the memory access pattern of logging is highly predictable, I originally expected the hardware L1 prefetcher to work well enough such that the number of L1 store misses is roughly the same as `LOG_ADDR`. However, for unknown reason, this is not the case. Therefore, I have to use software prefetch to load batches of log entries into L1 early enough. So far, I can only limit the effect of L1 store misses to ~0.75 cycles.

*Sixth, we can't afford to generate timestamp for every single event.* The overhead of `rdtsc` instruction is in the range of 25 to 35 cycles on modern CPUs, which means we probably have to amortize it over 64 or 128 events in a batch.

[NOTE]
==========================
During my experiment, I realized that there are many small factors that could affect the quality of the generated code (many of which are totally unexpected to me). Some examples are: C++ standard, GCC vs. Clang, `__thread` vs. `thread_local`, pre-increment vs. post-increment, condition test order, etc. Perhaps the takeaway here is that we should eventually write our logging function in optimized assembly code to avoid performance regression.
==========================

## ThreadSanitizer Performance

It's worth saying a bit more about the performance of TSan. One might conclude from its performance on `mini_bench_local` that its worst slowdown is ~*10X*. This is not true. The TSan function responsible for logging memory accesses, `MemoryAccess` (in `tsan_rtl.cc`), has several heavily optimized fast paths. In our example, all calls to this function returns early after `ContainsSameAccess` returns `true`; they never reach the function that accesses the VC state machine (i.e., `MemoryAccessImpl1`). Therefore, *10x* is only the worst slowdown of the *fastest* code path. So if a program updates shared memory locations a lot, it's going to hit the slow path of TSan frequently and results in much worse slowdown. In fact, we have seen examples where TSan results in over *1000x* slowdown.

# Evaluation

*TODO:* Probably have to wait till we have the real implementation
